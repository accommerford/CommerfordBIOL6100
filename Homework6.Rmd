---
title: "Homework 6"
author: "Audrey Commerford"
date: "2025-02-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Think about an ongoing study in your lab (or a paper you have read in a different class), and decide on a pattern that you might expect in your experiment if a specific hypothesis were true.

I recently found myself in a conversation about fluoridated water as a public health intervention. I am basing this model vaguely off of [this study](https://onlinelibrary.wiley.com/doi/full/10.1111/cdoe.12685#cdoe12685-bib-0010) done in Canada following the cessation of community water fluoridation in certain areas. The authors note that population exposure to fluoridation has decreased from 43% in 2007 to 39% in 2017. The expected pattern is that the prevalence of dental caries across a country will increase as population fluoride exposure decreases. 

2. To start simply, assume that the data in each of your treatment groups follow a normal distribution. Specify the sample sizes, means, and variances for each group that would be reasonable if your hypothesis were true. You may need to consult some previous literature and/or an expert in the field to come up with these numbers.

```{r}
# the explanatory variable is the percentage of population exposed to fluoridated water in a given region
exposure <- rnorm(50, mean=41, sd=10)
hist(exposure)
# the response variable is the prevalence of dental caries in the population
dental_caries <- rnorm(50, mean=3.1, sd=0.2)
hist(dental_caries)
```

3. Using the methods we have covered in class, write code to create a random data set that has these attributes. Organize these data into a data frame with the appropriate structure.

```{r}
# creating the data frame
data_frame <- data.frame(exposure, dental_caries)
print(data_frame)

# fitting the variables to a linear model
model <- lm(dental_caries~exposure)
```

4. Now write code to analyze the data (probably as an ANOVA or regression analysis, but possibly as a logistic regression or contingency table analysis. Write code to generate a useful graph of the data.

```{r}
# using anova to test for significance
anova(model)

# and qplot to plot the results
library(ggplot2)
qplot(x=exposure, y=dental_caries)
```

5. Try running your analysis multiple times to get a feeling for how variable the results are with the same parameters, but different sets of random numbers.

6. Now, using a series of for loops, adjust the parameters of your data to explore how they might impact your results/analysis, and store the results of your for loops into an object so you can view it. For example, what happens if you were to start with a small sample size and then re-run your analysis? Would you still get a significant result? What if you were to increase that sample size by 5, or 10? How small can your sample size be before you detect a significant pattern (p < 0.05)? How small can the differences between the groups be (the “effect size”) for you to still detect a significant pattern?


7. Alternatively, for the effect sizes you originally hypothesized, what is the minimum sample size you would need in order to detect a statistically significant effect? Again, run the model a few times with the same parameter set to get a feeling for the effect of random variation in the data.



8. Write up your results in a markdown file, organized with headers and different code chunks to show your analysis. Be explicit in your explanation and justification for sample sizes, means, and variances.



9. If you have time, try repeating this exercise with one of the more sophisticated distributions, such as the gamma or negative binomial (depending on the kind of data you have). You will have to spend some time figuring out by trial and error the parameter values you will need to generate appropriate means and variances of the different groups.

edit 
